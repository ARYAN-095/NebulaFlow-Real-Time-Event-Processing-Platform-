// aggregator/index.js
const { Kafka } = require("kafkajs");
const db = require("./db");
const { groupAndAverage } = require("./utils");

const kafka = new Kafka({ clientId: "aggregator", brokers: ["kafka:9092"] });
const consumer = kafka.consumer({ groupId: "aggregator-group" });

async function start() {
  await consumer.connect();
  await consumer.subscribe({ topic: "iot-sensor-data", fromBeginning: false });

  let buffer = [];

  // 1ï¸âƒ£ Set up the 1â€‘minute flush interval
  setInterval(async () => {
    console.log(`\nâ³ Flushing buffer; ${buffer.length} records collected`);
    if (buffer.length === 0) {
      console.log("âš ï¸  No data this interval, skipping aggregation");
      return;
    }

    const aggregates = groupAndAverage(buffer);
    buffer = [];  // reset for next window

    for (const record of aggregates) {
      try {
        await db.insertAggregate(record);
        console.log("âœ… Inserted aggregate:", record);
      } catch (err) {
        console.error("âŒ Failed to insert aggregate:", record, err);
      }
    }
  }, 60_000);

  // 2ï¸âƒ£ Consume messages into the buffer
  await consumer.run({
    eachMessage: async ({ message }) => {
      const raw = message.value.toString();
      console.log("ðŸ“¨ Received message:", raw);
      let data;
      try {
        data = JSON.parse(raw);
      } catch (e) {
        return console.error("âš ï¸ Invalid JSON, skipping:", raw);
      }
      buffer.push(data);
    },
  });
}

start().catch(err => {
  console.error("Aggregator crashed:", err);
  process.exit(1);
});
